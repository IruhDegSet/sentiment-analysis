{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preparation using vectorizer\n",
    "in this notebook we will use the sklearn vectorizer and bag of words\n",
    "## Bag of words\n",
    "bag of words, \"bow\", is a technic where we extract words from text as features, and ake from those words vectors \n",
    "*for example*:\n",
    "- \"this is good\"\n",
    "- \"good day\"\n",
    "- \"this is a long day\"\\\n",
    "when we vectorize those lines of text we get a bag of words which will contain a list of words **[this, is, good, day, a, long]** and for each line of the text the vectorizer will attribute the number of occurences of the word in the sentence\n",
    "the vectorizer out put will be:\\\n",
    "- [1 1 1 0 0 0]\n",
    "- [0 0 1 1 0 0]\n",
    "- [1 1 0 1 1 1]\\\n",
    "now if we try to vectorize a new line eg.:\\\n",
    "- '*this day is a good long day*'\\\n",
    "the output will be:\n",
    "- [1 1 1 2 1 1]\n",
    "\n",
    "if we enter a line with a whle new words, what will be the output?\\\n",
    "for eg.:\\\n",
    "'*I love cats*'\\\n",
    "the output: [0 0 0 0 0 0]\n",
    "\n",
    "look at it this way like a table, where the ***features*** are the words in given text and the rows contains the number of occurences in each sentence.\n",
    "| input lines | this | is | good | day | a | long|\n",
    "|:------|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| \"this is good\" | 1 | 1 | 1 | 0 | 0 | 0 |\n",
    "| \"good day\" | 0 | 0 | 1 | 1 | 0 | 0 |\n",
    "| \"this is a long day\" | 1 | 1 | 0 | 1 | 1 | 1 |\n",
    "| \"this day is a good long day\" | 1 | 1 | 2 | 1 | 1 | 1 |\n",
    "| \"I love cats\" | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "\n",
    "**Incovinients** is that order is not taken in consideration\n",
    "\n",
    "### n_grams\n",
    "the n_grams technic consists of taking n successive words instead of one word as a feature where n is the number of successive words:\n",
    "- 1_gram == monogram 1 word at a time\n",
    "- 2_grams == bigram 2 successive words at a time\n",
    "- 3_grams == trigram 3 successive words at a time\\\n",
    "this technic is used to keep sequences in dataframe\n",
    "\n",
    "with **bigram** the features will be **[this is, is good, good day, is a, a long, long day]** so the out put also will change insteade on counting the occurences of a word we will count the occurences of sub sequence\\\n",
    "the *output*:\n",
    "| line | this is | is good | good day | is a | a long | long day|\n",
    "|:------|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| \"this is good\" | 1 | 1 | 0 | 0 | 0 | 0 |\n",
    "| \"good day\" | 0 | 0 | 1 | 0 | 0 | 0 |\n",
    "| \"this is a long day\" | 1 | 0 | 0 | 1 | 1 | 1 |\n",
    "| \"this day is a good long day\" | 0 | 0 | 0 | 1 | 0 | 1 |\n",
    "| \"I love cats\" | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "\n",
    "**Incovinients** is that we loose some words if the exact same sub sequence don't appear in the new text\n",
    "\n",
    "### ngrams range\n",
    "the ngrams alone is not quite good so we use ngrams_range\\\n",
    "ngram_range is a tuple (s, e) where all the n_grams between s_gram and e_gram will be taken in consideration, inclusively\\\n",
    "ngram_range== (1, 4) --> 1_gram, 2_grams, 3_grams and 4_grams bag of words will be created\n",
    "\n",
    "for our example let's create an ngrams_range of (1,3)\\\n",
    "the bag of words will contain **[this, this is, this is good, this is a, is, is good, is a, is a long, good, good day, day, a, a long, a long day, long, long day]**\n",
    "\n",
    "the *output* table:\n",
    "| line | this| this is | this is good | this is a | is | is good | is a | is a long | good | good day | day | a | a long | a long day | long | long day |\n",
    "|:------|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| \"this is good\" | 1 | 1 | 1 | 0 | 1 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| \"good day\" | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 |\n",
    "| \"this is a long day\" | 1 | 1 | 0 | 1 | 1 | 0 | 1 | 1 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 |\n",
    "| \"this day is a good long day\" | 1 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 1 | 0 | 1 | 1 | 0 | 0 | 1 | 1 |\n",
    "| \"I love cats\" | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "\n",
    "**Inconvinients** the dataframe get larger quickly and imagine if there are spelling errors, abbreviations or sms writings like [lol, amaaaaazing, hhh, ] that will be features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "\n",
    "# for relative imports\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from src.data.text_2_dataframe import Text2DF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initalizing the vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words monogram\n",
    "CV1= Text2DF(vectorizer=True, ngrams_range= (1,1), use_wordlist= False, use_stopwords= False)\n",
    "\n",
    "# bag of words monogram using a vocabulary\n",
    "# using vocabulary is providing features at the instantiation\n",
    "CV2= Text2DF(vectorizer=True, ngrams_range= (1,1), use_wordlist= True, use_stopwords= False)\n",
    "\n",
    "# bag of words monogram using vocabulary and avoiding stopwords\n",
    "CV3= Text2DF(vectorizer=True, ngrams_range= (1,1), use_wordlist= True, use_stopwords= True)\n",
    "\n",
    "# bag of words monogram avoiding stopwords\n",
    "CV4= Text2DF(vectorizer=True, ngrams_range= (1,1), use_wordlist= False, use_stopwords= True)\n",
    "\n",
    "# bag of words bigram\n",
    "CV5= Text2DF(vectorizer=True, ngrams_range= (2,2), use_wordlist= False, use_stopwords= False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>my poor little dumpling in holmdel vids he was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>i m off too bed i gotta wake up hella early to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>i havent been able to listen to it yet my spea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>now remembers why solving a relatively big equ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>ate too much feel sick</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                              tweet\n",
       "0          0  my poor little dumpling in holmdel vids he was...\n",
       "1          0  i m off too bed i gotta wake up hella early to...\n",
       "2          0  i havent been able to listen to it yet my spea...\n",
       "3          0  now remembers why solving a relatively big equ...\n",
       "4          0                             ate too much feel sick"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>poor little vids trying hope dont try hard ton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>bed wake early tomorrow morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>able listen speakers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>now solving big equation total pain butt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>ate feel sick</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                              tweet\n",
       "0          0  poor little vids trying hope dont try hard ton...\n",
       "1          0                    bed wake early tomorrow morning\n",
       "2          0                               able listen speakers\n",
       "3          0           now solving big equation total pain butt\n",
       "4          0                                      ate feel sick"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data= pd.read_csv('../data/interim/cleaned_dataset.csv')\n",
    "filtered_data= pd.read_csv('../data/interim/filtered_dataset.csv')\n",
    "display(cleaned_data.head())\n",
    "filtered_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we gonna use:\n",
    "- cleaned version with vectorizer initialized with a vocabulary and/ or stopwords\n",
    "- filtered version with no vocabulary and no stop words\n",
    "\n",
    "to notice the difference\n",
    "the vectorizer with bigram bow will be used with filtered dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iruhdeg7/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['come', 'vis', 'viser', 'visest'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(ngram_range=(2, 2))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(ngram_range=(2, 2))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(ngram_range=(2, 2))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all the vectorizer with use_wordlist= False must be fitted to their text data\n",
    "CV1.count_vectorizer.fit(filtered_data['tweet'])\n",
    "CV4.count_vectorizer.fit(cleaned_data['tweet'])\n",
    "CV5.count_vectorizer.fit(filtered_data['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
